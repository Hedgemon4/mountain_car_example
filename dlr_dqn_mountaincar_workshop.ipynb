{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hedgemon4/mountain_car_example/blob/main/dlr_dqn_mountaincar_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74414de8",
      "metadata": {
        "id": "74414de8"
      },
      "source": [
        "# DLRL Summer School Workshop\n",
        "\n",
        "## Implementing DeepÂ Qâ€‘Networks (DQN) inÂ PyTorch\n",
        "\n",
        "Welcome\\! In this 30â€‘45â€¯ minute workshop youâ€™ll learn how to implement, train, **and actually watch** a DeepÂ Qâ€‘Network solve the classic control problem *MountainCarâ€‘v0*.\n",
        "\n",
        "This notebook is designed for those new to Deep Reinforcement Learning (DRL). We'll go step-by-step through the process of building a DRL agent, starting from the foundational concepts and culminating in a trained model that can successfully solve the task. We'll focus on not just *what* the code does, but *why* it's designed the way it is.\n",
        "\n",
        "**Learning objectives**\n",
        "\n",
        "1.  Understand the Gymnasium API and the MountainCar environment.\n",
        "2.  Code a random baseline agent and benchmark its performance.\n",
        "3.  Examine the environmentâ€™s state space and design a neuralâ€‘network encoder.\n",
        "4.  Implement the core building blocks of DQN:\n",
        "      * Replay buffer\n",
        "      * Target network\n",
        "      * Îµâ€‘greedy exploration\n",
        "5.  Train the agent endâ€‘toâ€‘end and visualise learning curves & rollâ€‘outs.\n",
        "6.  Briefly extend the solution with *frame stacking* and propose further experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ee20f1",
      "metadata": {
        "id": "60ee20f1"
      },
      "source": [
        "\n",
        "## 0Â â€”Â Setup\n",
        "\n",
        "Run the next cell **once** to install the libraries if youâ€™re in Colab or a fresh environment. If youâ€™re on an offline cluster that already has PyTorch & Gymnasium you can skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f25185",
      "metadata": {
        "id": "c3f25185"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install gymnasium[classic_control] torch numpy matplotlib tqdm\n",
        "!pip install moviepy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f73b756",
      "metadata": {
        "id": "4f73b756"
      },
      "source": [
        "### Imports & global configuration\n",
        "\n",
        "Here, we import the necessary libraries. We'll be using:\n",
        "\n",
        "  * `gymnasium` for the MountainCar environment.\n",
        "  * `torch` (PyTorch) for building and training our neural network.\n",
        "  * `numpy` for numerical operations.\n",
        "  * `matplotlib` for plotting our results.\n",
        "  * Other standard Python libraries for utilities.\n",
        "\n",
        "We also set a `SEED` for all random number generators. This is a crucial step in research to ensure that our results are **reproducible**. Anyone running this notebook should get the exact same results. Finally, we set up our device to use a GPU (`cuda`) if available, which will significantly speed up training. Otherwise, it will default to the CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d15c077a",
      "metadata": {
        "id": "d15c077a"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import gymnasium as gym\n",
        "import math, random, itertools, collections, os, copy, pathlib, sys, time, json\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Data manipulation and deep learning\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Plotting and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# --- Reproducibility helpers ---\n",
        "# Set a seed for all sources of randomness to ensure that the results are the same every time we run the code.\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# --- Device configuration ---\n",
        "# Check if a CUDA-enabled GPU is available for faster computation, otherwise use the CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425ddfb4",
      "metadata": {
        "id": "425ddfb4"
      },
      "source": [
        "## 1Â â€”Â The MountainCar Environment\n",
        "\n",
        "MountainCar is a *sparseâ€‘reward* task: a small car must build momentum to climb a hill. The agent receives **-1 reward** for each time-step it does not reach the goal. The goal is to reach the flag on the right hill. Because the car's engine is not strong enough to drive directly up the hill, it must learn to drive back and forth to build up enough momentum. This makes it a classic challenge in reinforcement learning.\n",
        "\n",
        "We'll use the `gymnasium` library, which provides a standardized interface for reinforcement learning environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9538b4d3",
      "metadata": {
        "id": "9538b4d3"
      },
      "outputs": [],
      "source": [
        "# Initialize the MountainCar-v0 environment.\n",
        "# \"render_mode\" is set to \"rgb_array\" so we can capture frames for visualization.\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "# The observation space defines the structure of the state received from the environment.\n",
        "# For MountainCar, it's a 2D vector: [position, velocity].\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "\n",
        "# The action space defines the set of possible actions the agent can take.\n",
        "# For MountainCar, there are 3 discrete actions: 0 (push left), 1 (do nothing), 2 (push right).\n",
        "print(\"Action space      :\", env.action_space)\n",
        "\n",
        "# Reset the environment to get an initial state.\n",
        "# env.reset() returns the initial observation and optional info.\n",
        "state, _ = env.reset(seed=SEED)\n",
        "state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482da077",
      "metadata": {
        "id": "482da077"
      },
      "source": [
        "## 2Â â€”Â Random Policy Baseline\n",
        "\n",
        "Before diving into complex algorithms like DQN, it's essential to establish a **baseline**. A simple, yet effective baseline is to see how well a completely random agent performs. This gives us a lower bound on performance; our sophisticated agent should, at the very least, perform better than random chance.\n",
        "\n",
        "We'll create a function `run_episode` that takes an environment and a policy function. The policy function decides which action to take given a state. For our random baseline, the policy will simply be to choose a random action at every step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5018916",
      "metadata": {
        "id": "f5018916"
      },
      "outputs": [],
      "source": [
        "def run_episode(env, policy_fn, render=False, max_steps=200):\n",
        "    \"\"\"\n",
        "    Runs a single episode in the environment following a given policy.\n",
        "\n",
        "    Args:\n",
        "        env: The Gymnasium environment.\n",
        "        policy_fn: A function that takes a state and returns an action.\n",
        "        render: If True, captures frames for rendering.\n",
        "        max_steps: The maximum number of steps per episode.\n",
        "\n",
        "    Returns:\n",
        "        The total reward accumulated during the episode and a list of frames if render=True.\n",
        "    \"\"\"\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    frames = []\n",
        "\n",
        "    # Loop for the maximum duration of an episode\n",
        "    for t in range(max_steps):\n",
        "        if render:\n",
        "            # Render the environment and store the frame\n",
        "            frame = env.render()\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Get an action from the policy function\n",
        "        action = policy_fn(state)\n",
        "        # Take the action in the environment\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        # An episode ends if the agent reaches the goal (terminated) or the max steps are reached (truncated)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    return total_reward, frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2adfa505",
      "metadata": {
        "id": "2adfa505"
      },
      "outputs": [],
      "source": [
        "# Define a random policy: it ignores the state and returns a random action.\n",
        "rand_policy = lambda s: env.action_space.sample()\n",
        "\n",
        "# Run 50 episodes with the random policy to get a stable average performance.\n",
        "returns = [run_episode(env, rand_policy)[0] for _ in range(50)]\n",
        "print(f'Random policy - average return over 50 episodes: {np.mean(returns):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "995c7775",
      "metadata": {
        "id": "995c7775"
      },
      "source": [
        "As you can see, the random policy performs poorly, never managing to reach the goal and accumulating a penalty of -200 (the maximum number of steps). Now, let's see what that looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a91d4b8",
      "metadata": {
        "id": "2a91d4b8"
      },
      "outputs": [],
      "source": [
        "# Render one random episode as a GIF to visualize its behavior.\n",
        "_, frames = run_episode(env, rand_policy, render=True)\n",
        "\n",
        "def display_frames_as_gif(frames, interval=40):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a GIF in the notebook.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(frames[0].shape[1]/80, frames[0].shape[0]/80), dpi=80)\n",
        "    plt.axis('off')\n",
        "    imgs = [[plt.imshow(f, animated=True)] for f in frames]\n",
        "    ani = animation.ArtistAnimation(fig, imgs, interval=interval, blit=True)\n",
        "    plt.close(fig) # Avoid displaying the static plot\n",
        "    display(HTML(ani.to_jshtml()))\n",
        "\n",
        "display_frames_as_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746f1914",
      "metadata": {
        "id": "746f1914"
      },
      "source": [
        "The car just wiggles around in the valley, which is what we'd expect. It never builds the momentum needed to climb the hill."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c1d6bab",
      "metadata": {
        "id": "4c1d6bab"
      },
      "source": [
        "## 3Â â€”Â Stateâ€‘Space Analysis\n",
        "\n",
        "The observation from the environment is a 2â€‘D vector containing the car's `[position, velocity]`. This is a continuous state space. Since neural networks are universal function approximators, they are well-suited to handle such continuous inputs.\n",
        "\n",
        "We will feed this 2D state vector directly into a small Multiâ€‘Layer Perceptron (MLP). The MLP's job will be to learn a mapping from any given state to the expected future rewards for each of the 3 possible actions. These expected rewards are called **Q-values**.\n",
        "\n",
        "Let's examine the range of values for position and velocity. This can be useful for normalization or for understanding the scale of our inputs, though for this simple MLP, it's not strictly necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45589a6d",
      "metadata": {
        "id": "45589a6d"
      },
      "outputs": [],
      "source": [
        "# Get the lower and upper bounds of the observation space\n",
        "obs_low, obs_high = env.observation_space.low, env.observation_space.high\n",
        "print('Position range :', obs_low[0], 'â†’', obs_high[0])\n",
        "print('Velocity range :', obs_low[1], 'â†’', obs_high[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8cc106",
      "metadata": {
        "id": "7f8cc106"
      },
      "source": [
        "## 4Â â€”Â Deep Qâ€‘Networks (DQN) Recap\n",
        "\n",
        "The core idea of DQN is to approximate the optimal actionâ€‘value function, **Q\\*(s,a)**, using a neural network. This function tells us the expected total future reward if we are in state *s*, take action *a*, and then continue to act optimally thereafter.\n",
        "\n",
        "Training a neural network with data generated from exploration in an RL environment is notoriously unstable. DQN introduces three key innovations to solve this:\n",
        "\n",
        "1.  **Replay Buffer**: This stores past experiences (state, action, reward, next\\_state). When we train the network, we sample random mini-batches from this buffer. This breaks the temporal correlation between consecutive samples, making the training data more like the independent and identically distributed (IID) data that neural networks are designed for.\n",
        "2.  **Target Network**: This is a separate, frozen copy of our main Q-network. It is used to calculate the target Q-values for our loss function. By keeping its weights fixed for a period of time, it provides a stable target, preventing the network from chasing a moving target, which can lead to oscillations and divergence.\n",
        "3.  **Îµâ€‘greedy Exploration**: This is a simple yet effective strategy to balance exploration (trying new actions) and exploitation (taking the best-known action). With probability Îµ, we take a random action; otherwise, we take the action with the highest Q-value according to our network. We typically start with a high Îµ and gradually decrease it as we learn more about the environment.\n",
        "\n",
        "The loss function for DQN is the mean squared temporalâ€‘difference (TD) error:\n",
        "\n",
        "$$L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim U(B)} \\left[ \\left( \\underbrace{r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a')}_{\\text{TD Target}} - \\underbrace{Q_{\\theta}(s, a)}_{\\text{Current Q-value}} \\right)^2 \\right]$$\n",
        "\n",
        "Where:\n",
        "\n",
        "  * $\\theta$ are the weights of our main Q-network.\n",
        "  * $\\theta^-$ are the weights of our stable target network.\n",
        "  * $(s, a, r, s')$ is a transition sampled from the replay buffer $B$.\n",
        "  * $\\gamma$ is the discount factor, which determines how much we value future rewards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f4ff786",
      "metadata": {
        "id": "9f4ff786"
      },
      "source": [
        "### Hyperparameters\n",
        "\n",
        "We will now define all our hyperparameters in one place using a `dataclass`. This makes it easy to see and modify the configuration for an experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705c79ca",
      "metadata": {
        "id": "705c79ca"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Args:\n",
        "    # --- Experiment ---\n",
        "    num_episodes: int = 1200\n",
        "    max_steps_per_episode: int = 200\n",
        "\n",
        "    # --- DQN Algorithm ---\n",
        "    lr: float = 1e-3\n",
        "    gamma: float = 0.99\n",
        "    buffer_capacity: int = 50_000\n",
        "    batch_size: int = 64\n",
        "    target_update_freq: int = 1000 # in steps\n",
        "    learning_starts: int = 1000 # in steps\n",
        "\n",
        "    # --- Epsilon-Greedy ---\n",
        "    eps_start: float = 1.0\n",
        "    eps_end: float = 0.01\n",
        "    eps_decay: float = 10000\n",
        "\n",
        "# Instantiate the arguments\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c44dbeae",
      "metadata": {
        "id": "c44dbeae"
      },
      "source": [
        "### The Q-Network Architecture\n",
        "\n",
        "The observation from the environment is a 2â€‘D vector containing the car's `[position, velocity]`. This is a continuous state space. We will feed this 2D state vector directly into a small Multiâ€‘Layer Perceptron (MLP). The MLP's job will be to learn a mapping from any given state to the expected future rewards (Q-values) for each of the 3 possible actions.\n",
        "\n",
        "We'll use a simple MLP with two hidden layers of 128 neurons each and a ReLU activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b747418",
      "metadata": {
        "id": "0b747418"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"A simple Multi-Layer Perceptron (MLP) to approximate Q-values.\"\"\"\n",
        "    def __init__(self, obs_dim, n_actions, hidden_sizes=(128, 128)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            obs_dim (int): The dimension of the observation space.\n",
        "            n_actions (int): The number of possible actions.\n",
        "            hidden_sizes (tuple): A tuple containing the size of each hidden layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last_dim = obs_dim\n",
        "        # Create the hidden layers\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(last_dim, h))\n",
        "            layers.append(nn.ReLU()) # ReLU is a common choice for activation function\n",
        "            last_dim = h\n",
        "        # Create the output layer\n",
        "        layers.append(nn.Linear(last_dim, n_actions))\n",
        "        # Combine all layers into a sequential model\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input state tensor.\n",
        "\n",
        "        Returns:\n",
        "            A tensor of Q-values for each action.\n",
        "        \"\"\"\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e8a660",
      "metadata": {
        "id": "d1e8a660"
      },
      "source": [
        "## 5 â€” Training the Agent\n",
        "\n",
        "Now we have all the pieces. Instead of wrapping everything in an `Agent` class, we will set up the components and the training loop directly in the script. This makes the flow of data and logic explicit and easier to follow.\n",
        "\n",
        "The training loop will:\n",
        "\n",
        "1.  Initialize the Q-Network, Target Network, Optimizer, and Replay Buffer.\n",
        "2.  Iterate for a specified number of episodes.\n",
        "3.  In each step of an episode:\n",
        "    a. Select an action using the Îµ-greedy policy.\n",
        "    b. Execute the action and observe the outcome (`next_state`, `reward`, `done`).\n",
        "    c. Store this transition in the replay buffer.\n",
        "    d. If enough steps have passed, sample a batch from the buffer and perform a learning update on the Q-Network.\n",
        "    e. Periodically update the target network weights.\n",
        "4.  Log the results for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ea6c22",
      "metadata": {
        "id": "f6ea6c22"
      },
      "outputs": [],
      "source": [
        "# --- 1. Initialization ---\n",
        "\n",
        "# Get observation and action space dimensions from the environment\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Main Q-network, which is actively trained\n",
        "q_net = QNetwork(obs_dim, n_actions).to(device)\n",
        "# Target network, a frozen copy of the q_net, used for stable target calculation\n",
        "target_net = QNetwork(obs_dim, n_actions).to(device)\n",
        "target_net.load_state_dict(q_net.state_dict())\n",
        "target_net.eval()  # Set the target network to evaluation mode\n",
        "\n",
        "# Optimizer for the q_net\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=args.lr)\n",
        "\n",
        "# Experience Replay Buffer using a deque\n",
        "replay_buffer = collections.deque(maxlen=args.buffer_capacity)\n",
        "\n",
        "# --- 2. Main Training Loop ---\n",
        "start_time = time.time()\n",
        "print(\"Starting training...\")\n",
        "\n",
        "# Lists to store metrics\n",
        "train_returns = []\n",
        "train_losses = []\n",
        "global_step = 0\n",
        "\n",
        "for ep in range(args.num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    ep_return = 0\n",
        "\n",
        "    for t in range(args.max_steps_per_episode):\n",
        "        # --- 3a. Select action with Epsilon-Greedy ---\n",
        "        # Calculate the current value of epsilon using an exponential decay formula\n",
        "        epsilon = args.eps_end + (args.eps_start - args.eps_end) * math.exp(-1. * global_step / args.eps_decay)\n",
        "\n",
        "        if random.random() < epsilon:\n",
        "            # Exploration: take a random action\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploitation: take the best action according to the Q-network\n",
        "            with torch.no_grad():\n",
        "                state_v = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                q_vals = q_net(state_v)\n",
        "                action = int(torch.argmax(q_vals).item())\n",
        "\n",
        "        # --- 3b. Execute action and observe outcome ---\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        ep_return += reward\n",
        "\n",
        "        # --- 3c. Store transition in replay buffer ---\n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        # Update the current state\n",
        "        state = next_state\n",
        "        global_step += 1\n",
        "\n",
        "        # --- 3d. Perform a learning step ---\n",
        "        if global_step > args.learning_starts:\n",
        "            # Sample a batch from the replay buffer\n",
        "            batch = random.sample(replay_buffer, args.batch_size)\n",
        "            states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "\n",
        "            # Convert numpy arrays to PyTorch tensors\n",
        "            states_v = torch.tensor(states, dtype=torch.float32, device=device)\n",
        "            actions_v = torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(1)\n",
        "            rewards_v = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "            next_states_v = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
        "            dones_v = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "\n",
        "            # --- Calculate the TD target ---\n",
        "            with torch.no_grad():\n",
        "                # Get the max Q-value for the next state from the target network\n",
        "                max_next_q = target_net(next_states_v).max(1, keepdim=True)[0]\n",
        "                # The TD target formula\n",
        "                target_q = rewards_v + (1 - dones_v) * args.gamma * max_next_q\n",
        "\n",
        "            # Get the Q-values for the actions that were actually taken\n",
        "            q_values = q_net(states_v).gather(1, actions_v)\n",
        "\n",
        "            # Compute the Mean Squared Error (MSE) loss\n",
        "            loss = F.mse_loss(q_values, target_q)\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Perform the optimization step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # --- 3e. Periodically update the target network ---\n",
        "            if global_step % args.target_update_freq == 0:\n",
        "                target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    train_returns.append(ep_return)\n",
        "    # Print progress every 20 episodes\n",
        "    if (ep + 1) % 20 == 0:\n",
        "        avg_return = np.mean(train_returns[-20:])\n",
        "        print(f'Episode {ep+1}/{args.num_episodes} â€” Avg Return (last 20): {avg_return:.1f} â€” Epsilon: {epsilon:.2f} â€” Steps: {global_step}')\n",
        "\n",
        "print(f'\\nTraining finished in {time.time() - start_time:.1f} s')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69786677",
      "metadata": {
        "id": "69786677"
      },
      "source": [
        "## 6Â â€”Â Visualizing Learning Progress\n",
        "\n",
        "After training, it's crucial to visualize the agent's performance. The most common way to do this is by plotting the return per episode. A rising curve indicates that the agent is learning. Since the returns can be very noisy from one episode to the next, we also plot a smoothed version of the curve to better see the underlying trend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638fc8f1",
      "metadata": {
        "id": "638fc8f1"
      },
      "outputs": [],
      "source": [
        "def smooth(y, w=25):\n",
        "    \"\"\"\n",
        "    Smooths a 1D array using a moving average.\n",
        "\n",
        "    Args:\n",
        "        y: The input array.\n",
        "        w: The window size for the moving average.\n",
        "\n",
        "    Returns:\n",
        "        The smoothed array.\n",
        "    \"\"\"\n",
        "    y = np.array(y, dtype=np.float32)\n",
        "    if len(y) < w:\n",
        "        return y\n",
        "    # Convolve with a uniform window to get the moving average\n",
        "    return np.convolve(y, np.ones(w)/w, mode='valid')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "# Plot the raw returns per episode\n",
        "plt.plot(train_returns, alpha=0.3, label='Episode return')\n",
        "# Plot the smoothed returns\n",
        "plt.plot(smooth(train_returns), label='Smoothed return (w=25)')\n",
        "# Plot the random baseline for comparison\n",
        "plt.axhline(np.mean(returns), color='r', linestyle='--', label='Random baseline')\n",
        "plt.title('Training Progress')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Return')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78a228a",
      "metadata": {
        "id": "f78a228a"
      },
      "source": [
        "The plot should clearly show the agent's performance improving over time, starting from the random baseline and reaching much higher returns as it learns a successful policy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba1362a",
      "metadata": {
        "id": "1ba1362a"
      },
      "source": [
        "## 7Â â€”Â Qualitative Evaluation\n",
        "\n",
        "Numbers and plots are great, but the best way to see if our agent has learned is to watch it in action\\! We'll run a few episodes using the trained agent, but this time, we'll turn off exploration (`training=False`). The agent will now always choose the action it believes is best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6df340",
      "metadata": {
        "id": "6b6df340"
      },
      "outputs": [],
      "source": [
        "def evaluate(agent, env, n_episodes=5, render=True):\n",
        "    \"\"\"\n",
        "    Evaluates the trained agent's performance.\n",
        "\n",
        "    Args:\n",
        "        agent: The trained DQNAgent.\n",
        "        env: The environment.\n",
        "        n_episodes: The number of evaluation episodes.\n",
        "        render: If True, captures frames for visualization.\n",
        "\n",
        "    Returns:\n",
        "        A list of returns and a list of video frames for each episode.\n",
        "    \"\"\"\n",
        "    rets, videos = [], []\n",
        "    for _ in range(n_episodes):\n",
        "        # The policy now uses training=False to ensure it's purely greedy (exploitative)\n",
        "        ret, frames = run_episode(env, lambda s: agent.policy(s, training=False), render=render)\n",
        "        rets.append(ret)\n",
        "        videos.append(frames)\n",
        "    return rets, videos\n",
        "\n",
        "# Run the evaluation\n",
        "eval_returns, eval_videos = evaluate(agent, env, n_episodes=5, render=True)\n",
        "print('Average return over 5 eval episodes:', np.mean(eval_returns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe277d1",
      "metadata": {
        "id": "0fe277d1"
      },
      "outputs": [],
      "source": [
        "# Display the first rollout as a GIF\n",
        "display_frames_as_gif(eval_videos[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb0528fa",
      "metadata": {
        "id": "cb0528fa"
      },
      "source": [
        "Success\\! The agent should now be able to consistently solve the MountainCar problem by effectively building momentum to reach the flag."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c08c28",
      "metadata": {
        "id": "c9c08c28"
      },
      "source": [
        "## 8Â â€”Â Ideas for Further Exploration ðŸŽ¯\n",
        "\n",
        "This notebook provides a solid foundation, but the world of Deep RL is vast. Here are some ideas for how you could extend this project:\n",
        "\n",
        "* **Algorithmic Improvements**:\n",
        "\n",
        "  * **Double DQN**: A small tweak to the TD target calculation that helps reduce the overestimation bias of Q-values, often leading to better performance. It uses the main network to select the best action and the target network to evaluate it:\n",
        "\n",
        "    $$\n",
        "    Y_t = r + \\gamma Q_{\\theta^-}\\left(s', \\arg\\max_{a'} Q_{\\theta}(s', a')\\right)\n",
        "    $$\n",
        "  * **Dueling DQN**: A neural network architecture that separates the estimation of state values, $V(s)$, and action advantages, $A(s, a)$. This can lead to better policy evaluation in the presence of many similar-valued actions.\n",
        "  * **Prioritized Experience Replay (PER)**: Instead of sampling uniformly from the replay buffer, PER samples transitions based on their TD error. This allows the agent to focus more on the experiences it was most surprised by, leading to more efficient learning.\n",
        "\n",
        "* **New Environments**:\n",
        "\n",
        "  * Try a **continuous** version of the environment, *MountainCarContinuous-v0*. In this version, the actions are continuous values, not discrete. Algorithms like **Deep Deterministic Policy Gradient (DDPG)** or **Soft Actor-Critic (SAC)** are designed for these types of action spaces.\n",
        "  * Scale up to a high-dimensional Atari game like **Breakout**. This will require using Convolutional Neural Networks (CNNs) to process the pixel inputs instead of an MLP. You'll also likely need to use **frame stacking** (combining several consecutive frames into one state) to give the agent a sense of motion.\n",
        "\n",
        "* **Experimentation and Tuning**:\n",
        "\n",
        "  * **Hyperparameter Search**: The performance of RL agents is often very sensitive to hyperparameters. Experiment with different values for the learning rate, epsilon decay schedule, network size, target update frequency, and replay buffer capacity.\n",
        "  * **Better Tooling**: For more serious experimentation, log your metrics (returns, losses, etc.) to a dedicated tool like **TensorBoard** or **Weights & Biases**. These tools provide much richer visualization and experiment tracking capabilities.\n",
        "  * **Code Optimization**: For a small speed-up, you can try using `torch.compile` (available in PyTorch 2.x and later) to Just-In-Time (JIT) compile your model for faster execution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91185483",
      "metadata": {
        "id": "91185483"
      },
      "source": [
        "## References\n",
        "\n",
        "  * *Humanâ€‘level control through deep reinforcement learning* â€”Â MnihÂ etÂ al., 2015. (The original DQN paper)\n",
        "  * OpenAI Gymnasium documentation: [https://gymnasium.farama.org/](https://gymnasium.farama.org/)\n",
        "  * PyTorch Reinforcement Learning tutorial: [https://pytorch.org/tutorials/intermediate/reinforcement\\_q\\_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}